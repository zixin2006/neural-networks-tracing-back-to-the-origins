{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Recurrent Neural Networks (RNNs)\n",
    "\n",
    "This notebook provides an introduction to some early examples of recurrent neural networks, including the Elman Network and the Jordan Network. I created an jupyter notebook file for here for the sake of clarity and comparison.\n",
    "\n",
    "## What is a Recurrent Neural Network (RNN)?\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to recognize patterns in sequences of data, such as time-series or text data. Unlike feedforward networks, RNNs have connections that loop back and this enables them to store information over time. \n",
    "\n",
    "## Early RNN Models\n",
    "- **Elman Network (1990)**: Introduced by Jeffrey Elman, this network includes context units to capture information from the previous step and maintains a \"memory.\"\n",
    "- **Jordan Network (1986)**: Introduced by Michael Jordan, this network is similar to the Elman network but includes feedback from the output layer instead of the hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Elman Network\n",
    "\n",
    "The Elman Network is a simple recurrent network with a feedback loop that allows information to be passed from the hidden layer to a set of **context units**. These context units then feed back into the hidden layer, allowing information from previous time steps to influence the network's current predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ElmanNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialization\n",
    "        self.W_xh = np.random.rand(input_size, hidden_size)\n",
    "        self.W_hh = np.random.rand(hidden_size, hidden_size)\n",
    "        self.W_hy = np.random.rand(hidden_size, output_size)\n",
    "        \n",
    "        # Context State\n",
    "        self.context = np.zeros(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden State\n",
    "        h = np.tanh(np.dot(x, self.W_xh) + np.dot(self.context, self.W_hh))\n",
    "        \n",
    "        self.context = h\n",
    "        \n",
    "        y = np.dot(h, self.W_hy)\n",
    "        return y\n",
    "\n",
    "# Sample\n",
    "elman_net = ElmanNetwork(input_size=3, hidden_size=4, output_size=2)\n",
    "input_data = np.random.rand(3)  \n",
    "output = elman_net.forward(input_data)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jordan Network\n",
    "\n",
    "The Jordan Network is another early RNN, similar to the Elman Network. Instead of feedback from the hidden layer, the Jordan Network has feedback from the output layer, which is stored in a set of **state units**. This difference gives it a unique approach to maintaining memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JordanNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.W_xh = np.random.rand(input_size, hidden_size)\n",
    "        self.W_hy = np.random.rand(hidden_size, output_size)\n",
    "        self.W_oy = np.random.rand(output_size, hidden_size)\n",
    "        \n",
    "        self.state = np.zeros(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden State\n",
    "        h = np.tanh(np.dot(x, self.W_xh) + np.dot(self.state, self.W_oy))\n",
    "        \n",
    "        y = np.dot(h, self.W_hy)\n",
    "        self.state = y\n",
    "        return y\n",
    "\n",
    "jordan_net = JordanNetwork(input_size=3, hidden_size=4, output_size=2)\n",
    "input_data = np.random.rand(3) \n",
    "output = jordan_net.forward(input_data)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Elman and Jordan Networks\n",
    "\n",
    "| Feature            | Elman Network                                 | Jordan Network                               |\n",
    "|--------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| Feedback source    | From hidden layer (context units)             | From output layer (state units)              |\n",
    "| Main advantage     | Useful for tasks needing hidden state memory  | Better for tasks where output influences next state |\n",
    "| Common applications| Time series analysis, sequence prediction     | Simple pattern recognition                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
