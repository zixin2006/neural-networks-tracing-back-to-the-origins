继上节 Cognitron 认知机（具备特征提取能力的神经网络）的讲解 ，我们继续来看一些能够处理语言的神经网络。这篇文章会讲解循环神经网络 (Recurrent Neural Network, RNN) 的起源之一： Elman 在1990年发表的论文 *Finding Structure in Time*，其中提出的架构通常被称为 Elman Network。Elman 网络提出的递归机制扩展了传统前馈神经网络的功能，使网络能够捕捉序列数据中的时间依赖性，为后续的长短期记忆网络 (LSTM) 和门控循环单元 (GRU) 都提供了关键的理论框架。

**动机：如何改善时间的表示问题？**

在过去的文章中，我们已经探讨了一些试图实现逻辑演算和视觉识别的神经网络（以及训练这些网络的算法）。然而，要真正模拟智能和认知，还有一个至关重要的因素——时间。

时间在认知中显然非常重要；它与许多表现为时间序列的行为，比如语言与连续帧理解密不可分。事实上，我们很难想象在没有某种时间前后关联性 (Time Dependency) 的情况下，如何处理诸如目标导向行为、计划或因果关系等基本问题。然而，在神经网络中引入时间这个观点似乎与当时普遍使用的并行式处理模型（例如 Hopfield Network 与 PDP 等）有所矛盾。在并行处理的范式下，每个隐藏层神经元的计算只取决于输入向量 $\mathbf{x}$ 的加权和，彼此之间没有时序的依赖关系，直接通过矩阵乘法完成。

不过，并行处理式的神经网络也不一定没有办法对序列信息建模，只是我们需要通过特定的设计和机制来实现。我们在下一个板块会继续讨论。

**Parallel Processing 处理序列时的一些问题**

一种就是将输入设计为固定长度的序列，将每一个时刻的信息与输入向量之间的元素对应。然而，这种空间化时间的方法存在多个显著的问题：
1. 忽略了时间序列的顺序和动态特征
2. 缺乏上下文记忆
3. 时间依赖建模的困难

为了解决这些问题，Elman 在 Perceptron 网络的基础上引入了context layer $U_h$，隐藏层的状态  $\mathbf{h}(t)$ 由当前输入  $\mathbf{x}(t)$  和前一时间步的隐藏状态（即上下文层中的内容）  $\mathbf{h}(t-1)$  共同决定：

$$
\mathbf{h}(t) = \sigma \left( \mathbf{W}_h \mathbf{x}(t) + \mathbf{U}_h \mathbf{h}(t-1) + \mathbf{b}_h \right).
$$

输出层的输出  $\mathbf{y}(t)$  由当前时间步的隐藏状态  $\mathbf{h}(t)$  决定：
$$
\mathbf{y}(t) = \sigma_o \left( \mathbf{W}_o \mathbf{h}(t) + \mathbf{b}_o \right).
$$

在每个时间步结束后，当前的隐藏状态  \mathbf{h}(t)  会被复制到上下文层中，用于下一时间步的计算：
$$
\mathbf{c}(t) = \mathbf{h}(t)
$$

Elman Network与其他的神经网络基本一样，都采用Backprop的方式来训练。下面我们会展示一些实验中Elman Network表现出来的能力，

**论文中的实验与结论**

1. 时序XOR问题：

- XOR（异或）问题是一个经典问题，无法通过简单的两层前馈神经网络解决。当这个问题被转化为时序形式时，网络需要预测每两个连续位的异或结果。
- 动机：探索递归神经网络（RNN）是否能够通过记忆（上下文层）处理基于时间的输入，并解决时序版本的XOR问题。
- 训练细节：网络接收3000个比特的输入序列，具有1个输入单元、2个隐藏单元、1个输出单元和2个上下文单元。任务是根据之前的比特预测下一个比特。网络在整个序列上训练了600次。
- 网络通过当前输入和之前隐藏层的状态（上下文）来学习预测下一个比特。相比静态XOR问题，网络在时序版本中开发了对重复或交替输入模式敏感的隐藏单元，解决方式有所不同。

2. 字母序列中的结构：

- 生成了一个基于规则的字母序列，其中辅音随机出现，而每个辅音之后是规律性的元音模式。
- 动机：测试网络是否能检测更复杂的时序模式，并基于输入序列的结构预测接下来的字母。
- 训练细节：输入序列由表示6个不同字母的6位向量组成。网络有6个输入单元、20个隐藏单元、6个输出单元和20个上下文单元。任务是预测序列中的下一个字母，网络训练了200个周期。
- 网络能够成功预测规律性的元音（它们遵循固定模式），但在预测随机分布的辅音时遇到困难。这表明网络能够根据输入中的规律性做出部分预测，即使在更复杂的依赖关系中。

3. 词预测任务（发现“单词”的概念）：

- 一个由字母组成的连续序列形成单词和句子，并且这些字母没有明确的单词边界。
- 动机：探索网络是否能够通过输入的时序结构，隐式学习“单词”的概念，而无需明确的单词边界信息。
- 训练细节：输入为4963个字母（由200个句子生成），每个字母用5位随机向量表示。网络有5个输入单元、20个隐藏单元、5个输出单元和20个上下文单元。任务是预测下一个字母。
- 网络的误差信号显示出单词之间的边界，新的单词开始时误差较高，而随着单词的预测变得更加可预测，误差逐渐减少。这表明网络能够通过共现统计数据隐式检测单词结构，即使网络没有明确被教导“单词”概念。

4. 从词序中发现词类：

- 句子中的词序反映了语法约束，但网络只能基于词的表面顺序进行学习。该实验的目的是研究网络是否能够仅根据词在句子中的顺序学习语法和语义类别。
- 动机：探索网络是否能够仅通过词序学习抽象的结构关系（如词类、语法）。
- 训练细节：一个句子生成器创建了27,534个词（由10,000个两词或三词的句子生成）。每个词用31位随机向量表示，网络有31个输入单元、150个隐藏单元、31个输出单元和150个上下文单元，任务是预测下一个词。
- 网络开发出能够捕捉词语语法类别（如名词、动词）的内部表示，并学习了词序的广义规律。隐藏单元激活的层次结构显示，网络能够区分不同类型的词（如有生命的名词与无生命的名词），并能够表示抽象的语法关系。

**总结**

实验表明，当问题被转化为时序形式时，解决方案的性质会发生变化。网络通过开发能够包含过去输入的记忆机制，成功处理并预测时序数据。网络利用误差信号来引导学习，尤其是在时序不规则或部分可预测的情况下。除此之外，网络开发了层次化和上下文敏感的表示，使其能够在处理类别之间的一般模式时也捕捉具体的实例。

