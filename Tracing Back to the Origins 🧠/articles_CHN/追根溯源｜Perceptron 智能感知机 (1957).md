写在前面：继上一篇追根溯源系列的专栏，我们继续探索MP神经元之后，在人工智能领域内的一项开创性工作「Perceptron 智能感知机」。本文章将会详细地解析这篇1957年发表的论文 *The Perceptron: A Probabilistic Model For Information Storage and Organization in the Brain*；作者是弗兰克 · 罗森布拉特，一位康奈尔大学的心理学&神经生物学家。

**简介**

文章伊始，作者提出了三个理解认知，泛化，回忆与思考的四个基本问题：

1. 生物系统如何感知或检测物理世界的信息？
2. 信息以何种形式存储记忆？
3. 存储的信息如何影响识别和行为？

第一个问题已经极大程度地在感知生理学的领域内解决。关于第二与第三个问题，作者主要讨论了两种立场：**编码记忆假设** (The Coded Memory Hypothesis) 认为信息是以编码的形式存储的；他们以以某种形式的代码或“布线图”存在，能够将感官刺激的模式直接转换为记忆。因此，认识任何外界刺激的过程都会涉及到将当前的感觉模式与存储的内容进行匹配或系统比较，再映射到一个相应的反应。**经验主义记忆假设** (The Empiricist Memory Hypothesis) 认为信息存储并非通过具体的编码，而是通过神经系统中新的连接或通道的形成。由于存储的信息以神经连接的形式存在，新的刺激将利用这些已经创建的路径，自动激活适当的反应，而不需要任何单独的过程来进行认识。图一是一个可视化：
![[Pasted image 20240728012843.png]]
<center>图一：编码记忆假设 (Symbolic) 与经验主义记忆假设 (Connetionist)。</center>

我们其实可以把这两种假设对应到两种主流的人工智能学派：联结主义 (Connectionist) 与符号主义 (Symbolic)。符号主义认为认知过程可以通过操作明确的符号和规则来实现，按顺序应用规则和操作，基于逻辑和形式化方法。联结主义主要通过模拟神经网络来理解和实现认知，使用分布式表示，依赖于统计和概率方法。

上一期专栏我们就介绍了基于布尔值与逻辑运算的神经网络：MP神经元。在Perceptron论文中，作者提出了符号方法的一些问题。这些理论家主要关注如何通过任何类型的确定性物理系统来实现感知和记忆等功能，而不是实际的大脑如何完成这些功能；已经提出的模型在几个重要方面都失败了：模型缺乏在不同情况下的一致表现（等效性），没有有效利用神经资源（神经经济性），对连接和同步的要求过于严格（过度特异性），模型中的变量或功能特征没有在生物学上得到验证或对应（缺乏已知神经关联的变量或功能特征）等。这些物理系统的支持者认为通过改进现有原理，就能够复现生物智能。**作者认为，这些缺点表明，无法对应生物系统的模型永远不能解释生物智能，因为原则上的不同是显而易见的。**

另一方面，更关注生物系统的研究通常在表述上不够精确，并且在分析中缺乏严格性——这会导致我们很难评估他们描述的系统是否能在真实的神经系统中工作，以及必要和充分条件是什么。**因此，缺乏熟练的，与布尔代数同等有效的分析语言也是障碍之一。**

为了解决这些问题，作者先引出了几个重要的假设：

1. 神经系统中最重要的初始网络的构造大多是随机的，仅受到最少数量的遗传限制。
2. 神经元连接具有一定可塑性，经过一段时间神经活动后，由于神经元内部的长期变化，刺激某组细胞导致另一组细胞反应的概率会改变。
3. 相似的刺激会趋于形成通向相同响应细胞的路径，反之亦然。
4. 正负强化（或起到这种作用的刺激）的应用会促进或阻碍正在进行的连接的形成。
5. 相似性并不是某些特定形式或几何类别刺激的必然属性，而是取决于感知系统的物理组织。

以上的假设会成为后续构建模型非常重要的基础。除此之外，需要注意，相较于之前的工作，**感知机的一大不同是选用了概率模型而不是布尔运算**。

**Perceptron的基础结构**

图二是感知机结构的一个简单示意图：
![[Pasted image 20240731010638.png]]
<center>图二：感知机结构的简单示意图。</center>
模型的组织分为四个部分：

1. **感知输入区**：感知机的输入来自于感受器（比如视网膜上的感受点，所以图上写的是retina）。我们将这些感受点称作S点 (Sensory points)，S点对接收到的刺激作出全有或全无的反应 (all-or-none)。注：“全有或全无”指的是，**若刺激的加权超过当前神经元的阈值** $\theta$，神经元被激活，没有超过则不被激活。
2. **投射区**：S点将信号传输到投射区（也就是关联细胞组 $A_1$，其中的神经元称作 $A$ 单元）$A$ 单元的激活同样基于“全有或全无”的原则。注意到这里的连接是 localized connection（集中联结）：**投射区中A单元的来源点（感知输入区中导致A单元激发的S点）倾向于围绕每个A单元对应的中心点集中或聚焦**；同时·，随着距离A单元中心点的视网膜距离增加，原点数量指数减少。这种分布模式在边缘检测中 (contour detection) 有重要的作用，因此是一个**仿生设计**。在建模过程中，投射区有时被省略。
4. **关联区**：每个关联区的A单元接收来自投射区的多个来源点，并且两个区域内的联结是**随机**的。
5. **响应层**：关联区的输出会传输到响应单元 (Response, 称作R单元)，每个响应单元都有一个特定的起始点集。值得注意的是，直到关联区之前感知机均是向前传输的，但是在响应层会给到前一层反馈。作者提出了两种近乎等效的反馈机制：
		(a) R单元对于自身来源集的细胞进行兴奋性反馈
		(b) R单元对于自身来源集补集的细胞进行抑制性反馈
	作者在后续处理中选择了机制 (b)。

在这样的系统中，基于反馈机制，神经元的响应是**互斥**的。如果一个响应单元 $R_1$ 被激活，它会抑制另一个响应单元 $R_2$ 的来源点集，继而抑制 $R_2$ 的响应。加强-抑制进行的过程中，系统在面对不同类别的刺激就能逐渐产生不同的反应——我们称这种通过**调节内部结构来改进响应的方式为学习能力**。

在后续的分析中，像上文所提到的，投射区这一步被忽略了；简化后的模型仅有**感知输入区-关联区-响应层**这三步，A单元指关联区的神经元，R单元指响应层的神经元。为了后续分析的简易性，作者区分了两个响应阶段，如图三所示：

![[Pasted image 20240801001757.png]]
<center>图三：对刺激的不同反应阶段。</center>

- 主导阶段 (Predominant phase)：系统中的A单元对刺激作出反应，但是R单元仍然不活跃。主导阶段是暂时的，会逐渐过渡到后主导阶段。
- 后主导阶段 (Postdominant phase)：系统中的一个响应单元变的活跃，并通过抑制其来源中的其他活动来主导反应。

在主导阶段，对刺激的反应是随机的。但在刺激-响应的连接不断得到加强之后，系统便产生了对特定刺激的响应——学习也就因此发生了。下面我们介绍宏观结构中每个神经元的特性。

**神经元特性建模**

为了回应假设二（神经可塑性）与实现Perceptron的动态学习过程，我们引入神经元特性的建模。这些工作会作为后续仿真的基础。

假设每个A单元输出的脉冲可以用一个值 $V$ 来表示，这个值可以是振幅、频率、延迟或完成传输的概率。如果一个A单元的值较高，那么它所有输出冲动都被认为是更有效的，或者更有可能到达响应层的。A单元对应的值被认为是一个相对稳定的特性（取决于细胞膜和细胞的代谢态），但这个值绝对不是恒定的。我们一般认为，**活动期会增加细胞的值，在非活动期会下降**。

还有一种有趣的系统，假设细胞在代谢材料上竞争，更活跃的细胞会以不活跃细胞为代价获得代谢材料。在这样的系统中，如果没有活动，所有细胞的状态将趋于相对恒定，不管是否有活动，整个系统的净值在某个时间点上都是平衡的。作者随后提出了三个不同系统内 ($\alpha, \beta, \gamma$) 的神经元 $V$ 值变化规则：

![[Pasted image 20240801140002.png]]
<center>图四：三种不同系统的逻辑特性比较。</center>

- $\alpha$ 系统 (Uncompensated Gain System)：活跃的A单元每次收到刺激都会收到一个固定的增益，并且这个增益是累积的，不会因为时间或其他因素而减少。累计增益机制是一种简单的学习方式，适用于长期累积和积累效应的任务。
- $\beta$ 系统 (Constant Feed System)：每一个来源集的增益以一个恒定的速率增加，增益的分配按来源集细胞的活动比例进行。除了主导集（系统的主要反应模式）中活跃的神经元，非主导集中的不活跃神经元也会获得增益。这样一来便能够确保系统能够学习并适应各种输出，避免某些单元永远不被激活和强化；恒定速率增益机制是一种平衡的学习方式，适用于稳定增益分配和持续学习的任务。
- $\gamma$ 系统 (Parasitic Gain System)：活跃的细胞以不活跃细胞为代价增加 $V$ 值，来源集的总值也因此保持恒定。寄生增益机制是一种竞争性学习方式，比较适合资源优化的任务等。

下一个部分我们会构建学习曲线并对模型进行敏感性分析。

**主导阶段的模型分析**

我们先定义两个来比较学习曲线的重要指标：

- $P_a$: 由某一给定大小的刺激激活的A单元的比例期望。我们通过对所有可能的激励 (e) 和抑制 (i) 来组合求和来计算这个期望比例，条件是**激励减去抑制的和达到或超过阈值** $\theta$ ($e-i\geq \theta$)。$P(e,i)$ 是激励和抑制成分的联合概率，$x$ 是每个A单元的激励连接总数，$y$ 则是每个A单元的抑制连接总数。$R$ 是A单元之前，S点被激发的比例。
$$\begin{align*}P_a&=\sum_{e=\theta}^x \sum_{i=0}^{\min (y,e-\theta)}P(e,i)\\ P(e,i)&=\binom{x}{e}R^e(1-R)^{x-e}\times \binom{y}{i}R^i(1-R)^{y-i}\end{align*}$$
- $P_c$: 一个对给定刺激 $S_1$ 产生反应的A单元也会对另一个给定刺激 $S_2$ 产生反应的条件概率
$$P_c=\frac{1}{P_a}\sum_{e=\theta}^x \sum_{i=e-\theta}^y \sum_{l_e=0}^e \sum_{l_i=0}^i \sum_{g_e=0}^{x-e} \sum_{g_i=0}^{y-i} P(e,i,l_e,l_i,g_e,g_i)$$
$P_c$ 计算的实际是 $e-i-l_e+l_i+g_e-g_i\geq \theta$ 的概率。其中，$l_e$ 与 $l_i$ 分别为当 $S_1$ 被 $S_2$ 替代后，A单元“丢失”的兴奋性和抑制性来源点数量；$g_e$ 和 $g_i$ 则分别代表当 $S_1$ 被 $S_2$ 替代后A单元“获得”的兴奋性和抑制性原点数量。$P(e, i, \ell_e, \ell_i, g_e, g_i)$ 的表达式如下：
$$P(e, i, \ell_e, \ell_i, g_e, g_i) = \binom{x}{e} R^e (1 - R)^{x - e} \times \binom{y}{i} R^i (1 - R)^{y - i} \times \binom{e}{\ell_e} L^{\ell_e} (1 - L)^{e - \ell_e} \times \binom{i}{\ell_i} L^{\ell_i} (1 - L)^{i - \ell_i} \times \binom{x - e}{g_e} G^{g_e} (1 - G)^{x - e - g_e} \times \binom{y - i}{g_i} G^{g_i} (1 - G)^{y - i - g_i}$$
$L$ 代表第一个刺激 $S_1$ 照亮的S点中未被第二个刺激 $S_2$ 照亮的比例，$G$ 代表从第一个刺激 $S_1$ 剩余的S点中包含在第二个刺激 $S_2$ 中的比例。虽然公式很复杂，但其实跟 $P_a$ 的计算思路基本一致，在此就不继续解释了（最重要的还是整体思路的把握）。现在来考察参数变化对 $P_a$ 和 $P_c$ 的影响。

**$P_a$ 的敏感性分析**

根据讲解完的公式与规则，可以写出代码绘图（代码见）。

![[Pasted image 20240801190302.png]]
<center>图五：Pa随着视网膜区域被照亮比例R的变化情况。</center>

根据图五，我们可以得出几个结论：
1. 增加阈值 $\theta$ 或者增加抑制性连接 $y$ 都会降低 $P_a$ 的大小（图五12）。
2. 当兴奋性和抑制性输入大致相等时，$P_a$ 随着 $R$ 的变化曲线趋于平缓（图五3）。
3. 通过比较，可以发现**兴奋性与抑制性输入的量级接近时，收敛速度会更快**。因此，为了优化网络在不同条件下的稳定响应，我们可能更希望兴奋性与抑制性输入。

**$P_c$ 的敏感性分析

![[Pasted image 20240801210509.png]]
<center>图六：Pc随R值的变化。</center>

1. 随着阈值 $\theta$ 的增加，$P_c$ 的值比 $P_a$ 的值下降得更快。
2. 与 $P_a$ 相同，$P_c$ 随着抑制连接 $y$ 的比例增加而减小。

除此之外，作者还研究了 $P_c$ 随刺激重叠比例 $C$（也就是刺激相似程度）的变化，如图七所示。
![[Pasted image 20240801211102.png]]
<center>图七：Pc随C值的变化。实线代表R=0.5，虚线代表R=0.2，x=10，y=0。</center>

1. $P_c$ 在刺激完全不重叠仍然大于0，这说明系统在两个刺激完全不相似的情况下仍然有一定概率响应。
2. 随着刺激的重叠程度增加，$P_c$ 的值趋向于1；这是一个好现象，说明系统响应的一致性较高。
3. 较高的阈值对应的 $P_c$ 值比较低的阈值对应的更低。

对 $P_a$ 与 $P_c$ 两个参数的考察为定量分析与参数调整提供了理论基础，我们可以通过这些结论不断提升神经网络的稳定性，并更好地理解其响应机制与表现。让我们来总结一下到目前为止的收获：

- 符号主义与联结主义
- 神经网络从形式逻辑到概率模型的过渡
- 感知机的基础结构与运行模式
- 神经元特性建模与三个系统
- 学习曲线构建与敏感性分析
- 算法基于不同指标的收敛情况

原论文的后半部分，作者主要探讨了感知器自发组织，在区分不同类别刺激时的学习和记忆能力，以及模型在随机与差异化环境中的表现。本文不会涉及的原因是，这部分研究虽然在理论上具有一定贡献，但主要集中在**小规模试验与精细调整**上，相较于前半部分的“开创性”工作可能具有一些局限性，尤其是从当下的视角来看。本篇文章，以及本系列的文章上，都会着重于介绍AI历史上一些开创性的，比较重要的工作。

Perceptron有一个比较大的问题是，它最多只能解决线性可分任务，对于非线性任务无能为力。这项工作之后，多层感知器 (Multilayer Perceptron) 逐渐被使用，通过在隐藏层中使用非线性激活函数来实现非线性分类。然而，神经网络的发展道阻且长，大型分类中的**训练问题**在此刻仍然没有一个合适答案。下一期我们将会继续讲解 Hinton, Rumelhart 和 Williams 于1986年发表的论文 *Learning representations by back-propagating errors*，探索反向传播算法是如何使训练深层神经网络成为可能的。

（最后的最后，创作不易，读到这里，还是求个点赞啦！（*＾3＾*））