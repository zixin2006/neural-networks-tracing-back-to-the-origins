上篇专栏讲到反向传播算法：是如何自动调整隐藏单元的状态，使神经网络的连接能够学习与调整状态，解决线性不可分问题的。现在，再介绍我们将介绍具备视觉能力（如特征提取功能）的神经网络，并探讨这些早期研究中所展现的卷积神经网络 (CNN) 的初步概念。这篇专栏会讲解1975年日本计算机科学家福岛邦彦发表的论文 *Cognitron: A Self-organizing Multilayered Neural Network* [1]，中文译为“认知机：一种自组织的多层神经网络。”如果想要快速了解论文的主要内容，可以直接读速通版。 

**速通版**

论文的主要创新点总结如下：
- 提出了一种新的神经元突触组织假设：**只有当一个神经元 $y$ 被某个神经元 $x$ 激活，并且附近的其他神经元没有比 $y$ 更强的响应时，连接的 $x$ 与 $y$ 的突触才会被加强**。
- 引入了**感受野** (receptive field) 的概念，采用二维的神经网络层，每个神经元通过其感受野接收来自前一层的输入信息，基于新假设比较激活强度与加强突触连接，逐层构建复杂的特征表示。
- 基于这些假设，作者推导出了一个有效组织多层神经网络的新算法，构建了名为认知机的自组织多层神经网络。Cognitron 的优势在于，能够使得网络能够在**无监督**的情况下，通过自动调整突触权重，在局部特征中提取更高层次的特征，实现复杂模式的自发识别能力。

下面的内容会对作者的动机，建模，与结果作详细的讨论。

**简介**

论文开篇就对**神经可塑性**这一概念作了阐述：大脑中神经元的突触连接并非完全由遗传决定，而是通过学习或出生后的经验进行可塑性改造的。Hubel 与 Wiesel 的研究指出，正常成年猫视觉皮层的神经元对视觉场中线条和边缘具有选择敏感性，并且不同神经元的偏好均匀分布在所有方向。然而，在完全由黑白条纹组成的环境中饲养的小猫，就没有神经元对与这些条纹垂直的方向做出反应 (Blake & Cooper, 1970)。这说明了缺乏某些视觉经验可能会导致神经元反应能力的缺失，**视觉皮层神经元的反应特性是在发育过程通过视觉经验调整的**。在神经网络中，这种天然的可塑性等价于**自组织**的实现，即网络在没有监督的情况下能实现权重更新，突触连接变化等。

读到这里，你可能会想到 Perceptron 中的神经元特性，他们看似已经能够自主实现突触连接调整。然而，因为最早的感知机只含有输入层，投影区，关联区与响应层四个部分，其中只有后两个部分的连接是随机可修改的，所以整个神经网络并不完全具有自组织能力。感知机在当时被寄予厚望，但事实证明它并不像起初期待的那样厉害。

![[Pasted image 20240818222607.png]]
图一：Rosenblatt于1957年提出的感知机 [2]。

那增加层数呢？我们都知道，如果增加神经网络的层数，它提取高阶信息的能力将大大增强。然而，在福岛邦彦撰写这篇论文的时候，没有任何一个能有效实现多层神经网络自组织的算法（当然，后来的反向传播算法实现了这一功能）。因此，那些声称是自组织的系统在根本上并没有超越三层感知机的框架。

为了解决这些问题，在根本上实现神经网络的无监督学习，作者提出了一个新的假设，对突触连接的强化进行建模。

**假设**

在介绍新假设之前，我们先看看先前假设中的问题，作者在论文中提到了三类：

- 突触 $c_i$ 在初始状态是可修改的，但如果出现突触后神经元 $y$ 没有在突触前神经元 $x_i$ 被激活的情况下被激活，那么 $c_i$ 会直接变为无效（不可再修改）。 
- 突触 $c_i$ 在初始时具有一定的随机成分，并且只有当突触前细胞 $x$ 与突触后细胞 $y$ 同时激活时，突触 $c_i$ 才会被强化。这种类型的突触被称为 Brindley 突触。
- 突触后细胞 $y$ 拥有另一个突触输入 $z$。这种额外的突触称为 Hebb 突触，初始时无效，只有在突触前细胞 $x$  激活并与控制信号 $z$ 同时作用时才会被强化。（该模型假设了 $z$ 作为“监督者”）

![[Pasted image 20240819175415.png]]图二：三种突触的具体机制。

第一种假设的问题是很致命的：哪怕网络只给出了一次错误信号，突触就会被不可逆地修改，甚至会导致网络的功能逐渐“灭绝”。Brindley突触虽然在一定程度上可以实现自组织，但是初始突出的随机性可能无法保证网络能够形成有意义的连接模式，并且也没有研究证明过大型网络中Brindley突触的有效性。Hebb突触假设了一个额外信号 $z$ 的存在，依赖外部监督者来修改突触连接，但是这样的存在生物学上并不合理。

为了解决这些问题，作者提出了一种新假设。神经元 $x$ 与 $y$ 之间突触 $c_i$ 的加强有两项必要条件：

1. 突触 $c_i$ 前神经元 $x$ 被激活。
2. 突触 $c_i$ 后神经元 $y$ 附近的其他神经元没有比 $y$ 响应更强

条件二意味着突触的强化在其邻域内具有唯一性。这种唯一性使网络中的每个神经元能够发展出自己的独特响应，增强了整个网络的**特征分化能力**，并且当某个神经元损坏时，其他神经元可以代替它的作用（类似生物神经网络的自我修复功能）。除此之外，新假设类似于大脑中营养物质仅集中供给响应最强的神经元的机制，具有生物学合理性。

下面我们先后对神经元与网络架构进行建模。

**神经元建模**

认知机的神经元采用了“**分流抑制**”（shunting inhibition）机制。在传统的线性抑制中，如果有一个兴奋性信号 $E$ 与抑制性信号 $I$，最终脉冲强度一般使用 $E-I$ 来表示。为了精细地调整神经元的响应，避免过度激活，分流抑制一般采用除法的形式。最终输出与实际神经元的行为类似，是与脉冲强度成比例的非负值（类似于激活函数）。记 $u(1), \ldots, u(N)$ 为来自兴奋性突触的输入，$v(1), \ldots, v(M)$ 为来自抑制性突触的输入，每个神经元的输出 $w$ 由以下公式定义：
$$w = \varphi \left[ \frac{1 + \sum_{v=1}^{N} a(v) \cdot u(v)}{1 + \sum_{μ=1}^{M} b(μ) \cdot v(μ)} - 1 \right]$$
其中， $a(v)$ 和 $b(μ)$ 是兴奋性和抑制性突触的传导率，取非负值。$\varphi[x]$ 定义如下：$$\varphi[x] = \begin{cases}
x & \text{当 } x \geq 0 \\
0 & \text{当 } x < 0
\end{cases}$$设 $e$ 是所有兴奋性效应的总和，$h$ 是所有抑制性效应的总和：
$$e = \sum_{v=1}^{N} a(v) \cdot u(v), \quad h = \sum_{μ=1}^{M} b(μ) \cdot v(μ)$$
神经元输出公式可以简写成：$$w = \varphi \left[ \frac{1 + e}{1 + h} - 1 \right] = \varphi \left( \frac{e - h}{1 + h} \right)$$在认知机中，突触传导率 $a(v)$ 和 $b(μ)$ 随着学习的过程逐渐增加（这是很符合直觉的一件事）；当突触的传导率增加且 $e \gg 1$ 且 $h \gg 1$ 时，以上公式可以近似写成：
$$w = \varphi \left( \frac{e/h - 1}{1/h} \right)$$
此时，输出由 $e/h$ 的比值决定，而不是由 $e$ 和 $h$ 的差值决定。可以看出，即使突触传导率随着学习而增加，只要兴奋性突触 $a(v)$ 和抑制性突触 $b(μ)$ 以成比例的速率增加，细胞的输出也会收敛到一个特定值而不会发散。我们假设兴奋性和抑制性输入成比例增加，写作：
$$e = \epsilon x, \quad h = \eta x$$
其中 $x$ 代表总信号强度。如果 $\epsilon > \eta$ 成立，神经元输出公式可以转换为（注1：$\tanh x=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{e^{2x}-1}{e^{2x}+1}$）：
$$\begin{align*}w = \frac{(\epsilon - \eta)x}{1 + \eta x} &= \frac{\epsilon - \eta}{2\eta}\cdot \frac{2\eta x}{1+\eta x}\\&=\frac{\epsilon - \eta}{2\eta}\cdot \left[1+\frac{e^{\ln \eta x}-1}{e^{\ln \eta x}+1}\right ]\\&=\frac{\epsilon - \eta}{2\eta} \left[1 + \tanh \left(\frac{1}{2} \ln \eta x \right)\right]\end{align*}$$
我们发现，这种输入-输出关系与Wever-Fechner定律所表达的对数关系一致，表现为由tanh表示的S形饱和响应。（注2：Weber-Fechner定律的核心要义是，人类感知的变化与物理环境的变化不成正比，而是遵循对数规律。举个例子，在一个安静的房间里轻轻放大音量会很容易察觉到声音变大了，但在吵闹的环境中，即使音量从很大增到更大，也可能几乎察觉不到变化。）

![[Pasted image 20240820134148.png]]
图三：Wever-Fechner定律——感知曲线与现实的差异。

我们得到的公式常常被用作神经生理学的经验公式，用来近似感官首体与动物整体感觉系统的非线性输入-输出关系。作者认为，由于这类神经元元件在特性上与生物神经元非常相似，它一定能适用于各种视觉与听觉信息处理系统。

**认知机结构**

基于新假设，我们开始介绍认知机的框架。认知机由类似结构的多个神经层级构成，这些层级按顺序排列。第 $l$ 层（标记为 $U_l$ ）由兴奋性神经元 $u_l(\mathbf{n})$ 和抑制性神经元 $v_l(\mathbf{n})$ 组成，其中 $\mathbf{n} = (n_x, n_y)$ 是代表神经元位置的二维坐标。

兴奋性神经元 $u_l(\mathbf{n})$ 接收来自位于 $U_{l-1}$ 中的兴奋性神经元 $u_{l-1}(\mathbf{n+v}) [\mathbf{v}\in S_l]$ 和抑制性神经元 $v_{l-1}(\mathbf{n})$ 的脉冲；其中，$S_l$ 表示细胞的可连接区域，$\mathbf{n+v}$ 表示 $S_l$ 区域内所有兴奋性神经元的坐标。如果我们将突触的传导率写为 $a(\mathbf{v, n})$ 和 $b(\mathbf{n})$，$u_l(\mathbf{n})$ 的输出可以用以下公式给出：$$u_l(\mathbf{n}) = \varphi \left[ \frac{1 + \sum_{v \in S_l} a(\mathbf{v, n}) \cdot u_{l-1}(\mathbf{n+v})}{1 + b(\mathbf{n}) \cdot v_{l-1}(\mathbf{n})} - 1 \right]$$抑制性神经元 $v_{l-1}(\mathbf{n})$ 接收来自前一层中邻近的兴奋性神经元 $u_{l-1}(\mathbf{n+v})$ 的信号，并对这些信号进行加权求和，再输出给 $u_l(\mathbf{n})$：$$v_{l-1}(\mathbf{n}) = \sum_{\mathbf{v} \in S_l} c_{l-1}(\mathbf{v}) \cdot u_{l-1}(\mathbf{n+v})$$其中 $c_{l-1}(\mathbf{v})$ 代表抑制性神经元的权重，权重的总和为1：$$\sum_{v \in S_l} c_{l-1}(v) = 1$$图四展示了 $U_{l-1}$ 和 $U_l$ 之间的连接具体情况。

![[Pasted image 20240820150801.png]]
图四：认知机结构可视化。

 可以看出，神经元 $u_l(\mathbf{n})$ 的 $S$ (可连接区域) 与连接到该神经元的兴奋性突触 $u_{l-1}(\mathbf{n})$ 的 $S$ 重叠。下面我们对连接强化机制作建模。基于假设，设 $\delta(\mathbf{n})$ 为一个布尔值函数，表示突触需要被强化与否；若$u_l(\mathbf{n})$ 的响应强于领域 $\Omega_l$ 范围内的所有神经元，则取值为1。$$\delta(\mathbf{n}) = \begin{cases}
1 & \text{if} \ \forall \ \mathbf{v} \in \Omega_l, \ u_l(\mathbf{n}) \geq u_l(\mathbf{n+v}) \ \\
0 & \text{otherwise}
\end{cases}$$当 $\delta(\mathbf{n}) = 1$ 时，$\Delta a(\mathbf{v, n})$ 和 $\Delta b(\mathbf{n})$ 的变化取决于 $u_l(\mathbf{n})=0$ 还是 $>0$。

 1. $u_l(\mathbf{n}) = 0$ $$\begin{align*}&\Delta a(\mathbf{v, n}) = q_0 \cdot c_{l-1}(\mathbf{v}) \cdot u_{l-1}(\mathbf{n+v}) \cdot \delta(\mathbf{n})\\ &\Delta b(\mathbf{n}) = q_0 \cdot v_{l-1}(\mathbf{n}) \cdot \delta(
\mathbf{n})\end{align*}$$
2. $u_l(\mathbf{n}) > 0$
$$\begin{align*}&\Delta a(\mathbf{v, n}) = q_1 \cdot c_{l-1}(\mathbf{v}) \cdot u_{l-1}(\mathbf{n+v}) \cdot \delta(\mathbf{n})\\

&\Delta b(\mathbf{n}) = \frac{\sum_{\mathbf{v} \in S_l} q_1 \cdot c_{l-1}(\mathbf{v}) \cdot u_{l-1}^2(\mathbf{n+v})}{2v_{l-1}(\mathbf{n})} \cdot \delta(\mathbf{n})\end{align*}$$
 $q_0$ 和 $q_1$ 是大于0的常数，且 $q_1 > q_0$。第一种情况，若 $u_l(\mathbf{n})=0$ 没有被激活且邻域内的神经元均无响应，突触强化的量较少。第二种情况，当 $u_l(\mathbf{n}) > 0$ 时，因为 $q_1 > q_0$，突触的强化会更显著，并且因为 $\Delta b(\mathbf{n})$ 被前驱层信号的**平方**压制了，使抑制性突触的强化量不会过度，展现出了假设中“胜者为王”的规则。

**算法定量分析**

作者基于认知机算法作了许多讨论，在这里我们不介绍详细的分析过程，但总结几个重要的insight：

1. 认知机经过学习后，强烈响应的神经元数量显著减少，表现出**稀疏性**，这可以帮助网络区分不同的输入模式。
2. 当 $u_l(\mathbf{n}) > 0$  时，兴奋性突触通常比抑制性突触强化得更强；而当 $u_l(\mathbf{n}) = 0$  时，抑制性突触的强化量可能更大。
3. 反复出现的相同刺激会使神经元 $u_l(\mathbf{n})$ 的输出之间增强。随着 $a(\mathbf{v,n})$ 逐渐增加，$$
\frac{\sum_{\mathbf{v} \in S_l} \Delta a(\mathbf{v, n}) \cdot u_{l-1}(\mathbf{n+v})}{\Delta b(\mathbf{n}) \cdot v_{l-1}(\mathbf{n})}=2
$$基于该式，回忆计算神经元输出的公式 $w = \varphi \left[ \frac{1 + \sum_{v=1}^{N} a(v) \cdot u(v)}{1 + \sum_{μ=1}^{M} b(μ) \cdot v(μ)} - 1 \right]$，可以近似得出 $w$ 会逐渐趋于1。这是很符合直觉的，这明网络已经“学习”到了特定的输出模式。

在基础算法之后，作者还对侧向抑制 (lateral inhibition) 现象作了额外的建模（因为重要性不优先，本文先不作介绍）。当某个神经元对特定刺激做出了强烈的响应时，侧向抑制会减弱周围神经元的响应，促进稀疏连接。

**网络层的连接与神经元分支**

最后，我们来定义网络层的连接方式。本篇论文讨论了三种不同的可连接区域确定方法。第一种方法使每层的可连接区域大小相等，但为了形成足够大的感知域，需要较多的层数来覆盖完整的输入层，导致网络结构复杂化。第二种方法随着层次加深逐渐增大可连接区域，尽管可以在较少层数下实现较大的感知域覆盖，但这种方法会导致最后一层（输出层）神经元的响应过于相似，削弱了网络对不同刺激的区分能力。

![[Pasted image 20240820195005.png]]
图五：三种网络层连接方法。

第三种方法（5c），即本文采用的方案，使轴突在经过层间传递时分成多个概率性分布的分支，既能扩大感受野覆盖范围，又避免了过度重叠的问题。

认知机采用了第三种方法。假设每个兴奋性神经元 $u_l(\mathbf{n})$ 的轴突分成 $K+1$ 个支线，这样原来的表示就可以改写成 $u^{\prime}_l(\mathbf{n}, k)  \left[ k = 0, 1, 2, \dots, K \right]$。假设 $k = 0$ 的分支没有偏移：$$u^{\prime}_l(\mathbf{n}, 0) = u_l(\mathbf{n})$$另一方面，$k > 0$ 的分支目标位置会产生一定变化，并且越接近原始位置的变换越有可能发生。设 $P_{lk}$ 为置换 $\mathbf{n}$ 的算子。我们有：
$$u^{\prime}_l(\mathbf{n}, k) = P_{lk} \{ u_l(\mathbf{n}) \} \quad (k \neq 0)$$
在计算机模拟中，本篇论文研究了 $K = 1$，即轴突分成两个分支的情况，其中一个分支直接向前传递，另一个分支接收概率性方位置换。

轴突分支的神经元输出可以重新写为：
$$u^{\prime}l(\mathbf{n}) = \varphi \left[ \frac{1 + \sum_{k=0}^{K} \sum_{v \in S_l} a(\mathbf{v, n}, k) \cdot u^{\prime}_{l-1}(\mathbf{n+v}, k)}{1 + b_l(\mathbf{n}) \cdot v_{l-1}(\mathbf{n})} - 1 \right]$$
其他公式大体保持不变，只进行以下变量的替换即可：
$$\begin{align*}u_{l-1}(\mathbf{n+v}) &\rightarrow u^{\prime}_{l-1}(\mathbf{n+v}, k)\\ a(\mathbf{v, n}) &\rightarrow a(\mathbf{v, n}, k) \\ c_{l-1}(\mathbf{v}) &\rightarrow c_{l-1}(\mathbf{v}, k) \\ \sum_{\mathbf{v} \in S_l} &\rightarrow \sum_{\mathbf{v} \in S_l} \sum_{k=0}^{K}\end{align*}$$
Now we're ready to see the results!

**计算机模拟实验方法与结论**

在模拟实验中，作者使用了四个网络层，每层包含 $12\times12$ 个兴奋性神经元和相同数量的抑制性神经元。网络层之间的连接区域 $S$、邻近区域 $\Omega$ 以及侧向抑制的范围均进行了详细设定。实验向认知机的输入层 $U_0$ 依次呈现了0到4的 $12\times12$ 数字图片，并记录了每个周期中各层神经元的响应情况。

![[Pasted image 20240820204017.png]]
图六：0-4 数字响应情况。

研究发现，认知机经过多次刺激后能够成功地形成自组织，尤其是在最深层（$U_3$ 层），大多数细胞能够选择性地响应某一特定的刺激模式。（结果见图六）

![[Pasted image 20240820204446.png]]
图七：（左）从单层细胞的正常反应中进行逆向再现。第一行显示了对刺激“4”的正常反应，该反应是在模式呈现第20个周期时记录的。第二行、第三行和第四行分别是从 $U_1, U_2$ 和 $U_3$ 层的正常反应模式中逆向再现的结果。（右）从单个细胞的正常反应中进行逆向再现。第一行显示了对刺激“4”的正常反应。第二行、第三行和第四行分别是从 $U_1, U_2$ 和 $U_3$ 层中反应最强烈的单个细胞的正常反应中逆向再现的结果。

为了验证突触组织的效果，研究者还进行了逆向再现实验，即假设信息流通过突触的方向是逆向的，从而观察各层细胞的响应情况。结果表明，通过这种方法，不仅可以**从144个神经元的响应中推测出具体数字，也可以从较深层的单个细胞反向推断出较浅层的输入模式**，这表明每个神经元的响应具有独特性，认知机在这个特定的任务上发展出了自组织能力。（结果见图七）

![[Pasted image 20240820205246.png]]
图八：认知机在接受相似刺激模式时的网络层与单神经元逆向推理能力。

在最后一组实验中，作者探讨了认知机对相似刺激模式（如“X”、“Y”、“T”和“Z”）的响应能力，用上文同样的测试方法开展试验。结果表明，即使这些字母具有共同成分（X与Y的头部，Y与T的尾部等），认知机仍能够区分并作出不同的响应，表明其在处理相似信息时具有一定的分辨能力。

我们来总结一下本篇论文的成果：通过遵循一种新的突触组织假设（胜者为王，败者为寇！（大雾）），认知机成功地实现了自组织学习，算法的许多方展现出了与生物脑相似的特性。由于认知机具有多层结构，相较于传统的脑模型或先前提出的人工神经网络来说，它能够在信息处理方面实现复杂的任务。然而，研究也指出，认知机并不具备完整的模式识别功能。举个例子，如果要让认知机能够全面地执行模式识别任务，还需要引入额外的功能，如空间模式的规范化或补全（这有点类似于我们想要实现AGI的目标，事实证明，即使到2024年，前方的路仍然道阻且长）。

Cognitron之后，作者福岛邦彦还有另外一篇论文叫做Neocognitron，是认知机的升级版，使得神经网络的识别不受图像旋转等因素印象，感兴趣的话也可以自行阅读！

**结语**

「追根溯源」系列的更新已经逐渐接近尾声，在下一期关于最早语言模型的论文 *A Neural Probabilistic Language Model*，以及下下期的论文代码复现之后就将结束。在撰写这些文章的过程中，作者也发现了许多有意思的趋势：越早期的工作越强调“生物脑”与“人工脑”的对应，比如在Cognitron的论文里，就能够在四处看到神经生物学的影子，并且take advantage of it。上一期的Backpropagation离我们现在更近一点，这样的对应就已经少了许多，但作者仍然在文末留下了“生物脑并不存在反向传播机制，所以我们应该找到更符合情理的学习算法”这样的评论。至今，人工智能领域内的研究已经大多归类到纯计算机门类下，寻找novel的工程想法，在各种项目中刷SOTA，却极少有解答“为什么work？”问题的工作。这样的差异有时不得不令人产生疑问：我们研究的目的是否与过去产生了差异？我们是否在实现AGI/智能的道路上呢？我们先在这里留一个question mark，且看时间怎么说。

最后的最后，创作不易，读到这里，还是求个点赞啦！（*＾3＾*）

**References**

[1] Fukushima, K. (1975). Cognitron: A self-organizing multilayered neural network. _Biological Cybernetics_, _20_(3–4), 121–136. https://doi.org/10.1007/bf00342633

[2] Singh, K., Ahuja, A., Chatterjee, T., Pritam, S., Varma, N., Jain, R., Sachdeva, M. S., Bansal, D., Trehan, A., Hajra, S., Kar, R., Basu, D., Peepa, K., Anil, I., Banashree, R., Apabi, H., Ghosh, S., Samanta, S., Chattopadhay, A., & Bhattacharyya, M. (2019). 60th Annual Conference of Indian Society of Hematology & Blood Transfusion (ISHBT) October 2019. _Indian Journal of Hematology and Blood Transfusion_, _35_(S1), 1–151. https://doi.org/10.1007/s12288-019-01207-5
