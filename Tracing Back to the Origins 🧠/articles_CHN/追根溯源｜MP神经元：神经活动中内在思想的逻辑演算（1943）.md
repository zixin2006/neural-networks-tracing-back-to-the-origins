写在前面：这篇文章会讲解1943年生物学家Warren McCulloch与逻辑家Walter Pitts发表的论文《A Logical Calculus of the Ideas Immanent in Nervous Activity》[1]，译成中文为“神经活动内在思想的逻辑演算”。建议阅读这篇文章前，先阅读：，以更好的理解论文中使用的符号系统。笔者近期在读一些“古老”的论文（它们通常被认为是当代人工智能发展的根基），然而苦于内容生涩且未见详实解析，利用闲暇时间差不多摸清了此类文章的脉络，希望“追根溯源”的文章能够帮到有类似兴趣却无从下手的学习者(^-^)

本篇文章5000字左右，阅读时长大概20分钟。

如果我们现在正在上一节人工智能通识课，我想老师很有可能会从人工神经网络（Artificial Neural Network, ANN）讲起，并配以下图右边的结构来告诉我们什么是输入层、隐藏层、与输出层。神经元输出的加权求和，再通过一个激活函数 $f$ 传递到下一层的计算，似乎很符合我们关于仿生结构的直觉，也符合神经元产生脉冲的大致机理。然而，科学家们是如何从将神经活动的复杂回路（图一左）抽象成简洁的ANN呢？发表于1943年的《A Logical Calculus of the Ideas Immanent in Nervous Activity》则是回答这个问题，广为认可的最早的相关工作。这篇文章主要的贡献主要在于提出了一个理想化的基于逻辑运算的神经元模型，使神经元能够通过阈值响应来处理信息，并且证明了神经网络可以模拟任何机械计算过程以及其宏观视角下的不变性。一言以蔽之，如图一所描绘的那样，就是将神经生理过程的复杂性大大简化为逻辑命题的形式。

![[Pasted image 20240710012447.png]]
<center>图一：（左）谷歌于2021年映射的人脑中一小部分的一个高分辨率细节图像 [2]；（右）神经网络的基本结构 [3]。</center>

**简介**

文章首先在Introduction中抛出了几个观点（在这里不逐字分析，感兴趣可以上网检索pdf；事实上因为论文内容比较久远，读起来还是有点费劲的，尤其是对于非生物生，比如我）

1. 神经活动具有“all-or-none”的性质。（神经活动要么就有要么就没有，可以看作是0或1，true or false的判断）
2. 为了使下一个神经元产生脉冲，必须要有一定数量的突触在一定时间内激活。（翻译一下就是没个神经元都具有独立于其位置与先前活动的阈值）
3. 神经系统中唯一显著的延迟就是突触传导延迟。（轴突延迟忽略不计）
4. 抑制突触阻止了神经元在某些时刻的激活。（利用抑制突触解释神经元的不应期）
5. 神经网络的结构不随着时间变化。（Learning与Extinction等长期发生的现象在广义上与原来的网络等价）

基于这些假设，我们开始定义一些符号。若看不懂这一部分的表达式，可以先阅读前置文章（重复x2）。

**定义**

我们首先定义一个函子$S$，其对于某个性质P的值是当P满足其前驱时所持有的属性，具体表达式为$$S(P)(t)=P(Kx), t=x^{'}$$其中的 $K$ 是一个数值算子，$t$ 表示 $x$ 的后继；这个式子看起来很复杂，然而实际上的意思就是 $S(P)(t)=P(t-1)$. 在这里，左边括号里的参数经常会被省略，从而写成一个谓词，$[\textbf{Pr}]$ 且 $S^2 \mathbf{Pr}=S(S(\textbf{Pr}))$. 

现在，我们对神经网络的具体构成进行定义。一个神经网络 $\mathcal{N}$ 包含神经元 $c_1, c_2, \ldots, c_n$；$N_i(t)$ 代表了神经元 $c_i$ 在 $t$ 时刻的状态，描述该神经元是否在 $t$ 时刻触发。我们有时将其归为对象语言，写成 $N_i$ 的形式。定义 $\mathcal{N}$ 的周围神经元为没有轴突与之突触连接的神经元。**周围神经元没有轴突与之突触连接，因此他们的输入信号直接作用于网络的其他部分，不受网络内部其他神经元的影响（一个恰当的形容是“输入”），我们从而可以利用该性质将复杂的神经活动分解成更简单的子部分来理解**。 我们令 $N_1,\ldots,N_p$ 表示这些神经元的作用，并令 $N_{p+1},\ldots,N_{n}$ 表示其余的神经元。这样一个神经网络的解是有如下形式的一系列句子：$$S_i: N_{p+1}(z_1)\equiv Pr_i(N_1,\ldots,N_p,z_1)$$其中 $Pr_i$ 只包含一个自由变量 $z_1$，可能还包括一些常量句子 $[\mathbf{sa}]$；并且每个 $S_i$ 都对 $\mathcal{N}$ 成立。下面，我们定义两个重要的术语：狭义实现性与广义可实现性。
![[Pasted image 20240711215200.png]]<center>图二：狭义可实现(realizable in the narrow sense, i.n.s.)与广义可实现(realizable in the extended sense)的原文定义部分 [1]。</center>

与求解相反地，考虑一个谓词 $Pr_1(^1{p_1}^1, ^1{p_2}^1, \ldots, ^1{p_p}^1, z_1, s)$，我们说它是狭义可实现的，如果存在一个网络$\mathcal{N}$与其中的一系列 $N_i$，使得 $N_1(z_1)\equiv Pr_1(N_1,N_2,\ldots,z_1,sa_1)$ 成立。继而，我们定义广义可实现性为，对于一些 $n$, $S^n(Pr_1)(p_1,\ldots,p_p,z_1,s)$ 狭义可实现。

我们对于上述定义做一些解读。狭义可实现是一个较为严格的条件，要求直接在某个给定的神经网络结构中，通过特定的神经元排列和输入替换，来实现特定谓词描述的状态。相对的，广义可实现性则是一个较为宽松的条件，允许通过多次应用函子 $S$ 来变换和扩展命题，直到这个扩展后的命题在狭义上是可实现的。

![[Pasted image 20240711224339.png]]
<center>图三：一个简单的神经网络。</center>

我们在这里举一个简易的例子（图三）。 先考虑表达式 $N_3(z_1)\equiv Pr_(N_1,N_2,z_1)$：因为 $N_3(t)\equiv N_1(t-1)\land N_2(t-1)$，根据i.n.s.的定义，这个神经网络是狭义可实现的。我们再考虑 $N_5(z_1)\equiv Pr(N_1,N_2,z)$：这个表达式无法直接实现，但是我们可以巧妙地利用间接神经元来得出一个神经网络。因为 $N_5(t)\equiv N_4(t-1)\lor N_3(t-1)$；同时，根据 $N_3(t)\equiv N_1(t-1)\land N_2(t-1)$，可以得出 $N_5(t)\equiv N_1(t-2)\land N_2(t-2)$. 因为 $S^2Pr$ 狭义可实现，所以 $Pr$ 广义可实现，如图三的神经网络所示。（事实上在写完文章之后我发现应该 $c_4$ 把移动到 $c_5$ 的位子上并且删除 $c_5$ 就能讲清楚这个问题，不过关系不大啦。）

我们终于前进到了最后一个定义！论文采用了递归规则定义了时序命题表达式（TPE）。
![[Pasted image 20240711232505.png]]
<center>图四：时序命题表达式的递归定义 [1]。</center>

首先，$p[z_1]$ 是最基本的时序命题表达式，$p_1$ 是一个谓词变量，$z_1$ 是某个时刻。其次，定义提出，对TPE $S_1$与$S_2$ 通过函子 $S$，逻辑“或”，“与”，或者“否”操作后得出的结果仍然是TPE。最后，除了上述情况，没有其他形式可以构成TPE——这确保了定义的封闭性与完整性。

**回顾**

我们对函子$S$，神经网络的解，狭义与广义可实现性，与TPE做了定义与大致介绍。在开启证明与示例前，先回顾一下核心问题：
1. 找到一种有效的方法来获取构成任何给定网络解的一组可计算的 $S$（计算任何网络的行为）。
2. 描述一组可实现的解。
简单的来说，问题是（1）计算任何网络的行为（2）确定呈现为某种状态的网络。

**定理与简易证明**

*注1：0阶神经网络指不包含环形结构的网络。论文的后半部分对net with circles的理论作了许多解释，但这篇文章不会涉及。*

Theorem 1. *Every net of order* 0 *can be solved in terms of temporal propositional expressions.*
定理一：所有0阶神经网络均可以用TPE（时序命题表达式）来求解。

让 $c_i$ 为 $\mathcal{N}$ 中的任意一个神经元，这个神经元阈值为 $\beta_i > 0$。令 $c_{i1}, c_{i2}, \ldots, c_{ip}$ 分别在 $c_i$ 上有兴奋性突触 $n_{i1}, n_{i2}, \ldots, n_{ip}$。令 $c_{j1}, c_{j2}, \ldots, c_{jq}$ 在 $c_i$ 上有抑制性突触。令 $\kappa_i$ 为 $\{n_{i1}, n_{i2}, \ldots, n_{ip}\}$ 的子集类，条件为使得这些字集的求和超过 $\beta_i$（能够激活 $c_i$）。我们继而可以根据简介部分提出的假设写出:$$N_i(z_1)\equiv S\left \{\prod_{m=1}^q \lnot N_{jm}(z_1) \land \sum_{\alpha\in \kappa_i}\prod_{s\in \alpha}N_{is}(z_1)\right \}$$在这之中，$\sum$ 与 $\prod$ 分别代表有限的析取与合取。因为论文的年代比较久远，这一套表达式可能看起来很复杂，但它的含义转译一下实际很简单，即：神经元 $c_i$ 在 $z_1$ 时刻放电等价于在 $z_1-1$ 时刻，前驱的任意一个抑制性神经元都没有放点，并且存在一组脉冲值加和超过阈值 $\beta_i$ 的兴奋性神经元，如图五所示：

![[Pasted image 20240716170718.png]]
<center>图五：兴奋性与抑制性神经元构成的神经网络。</center>

假设阈值为二，可以很明显地看出，$N_4$ 放电的条件为 $N_1$ 与 $N_2$ 均为真（$c_1$ 与 $c_2$ 在前一个时刻放电），并且 $N_3$ 为假（$c_3$ 这个抑制性神经元不被触发）。因为以上的表达式对于每个非周围神经元都能够写出，我们可以不断通过不断地替换 $N_{jm}$ 与 $N_{is}$ 地等价表达式，直到  $N_i(z_1)$ 可以完全等价于一个由周围神经元与构成的命题逻辑表达式，这样我们就得出了一个神经网络的解。

Theorem 2. *Every TPE is realizable by a net order of zero.*
定理二：所有TPE均可以用0阶神经网络（广义）实现。

*注2：原论文提到 realizable in the extended sense 在证明过程中简写为 realizable。*

第二个定理告诉我们，对于任意一种神经元状态的描述，都可以找到一个0阶神经网络来实现；定理二的论证过程实际提供了一个通过递归构建0阶网络来实现TPE的方法。

首先，因为函子 $S$ 实际上是一个时间操作符，它与基本的逻辑运算（析取、合取、否定）是可交换的。这样一来，如果神经网络 $\mathcal{N}$ 可以在当前时间点实现 $S_i$，我们可以通过添加适当的延时神经元，使得这个神经网络可以在前一个时间点实现 $S_i$ 的状态。来看一个例子：

假设有一个命题 $S_i=p_1(z_1)\lor p_2(z_1)$，并且$S_i$ i.n.s。应用函子 $S$ 以及它合逻辑运算的可交换性，得到$$S(S_i)=S(p_1(z_1)\lor p_2(z_1))=S(p_1(z_1))\lor S(p_2(z_2))$$$S$ 是一种时间操作符，$S(p_1(z_1))$ 与 $S(p_2(z_2))$ 则形容 $p_1(z_1)$ 与 $p_2(z_1)$ 在前一个时刻的状态。通过延展神经网络 $\mathcal{N}$，引入适当的延时神经元，则可以在狭义上实现 $SS_i$。

![[Pasted image 20240717173651.png]]
<center>图六：通过使3号神经元变为间接神经元来实现神经网络。</center>

由此，如果某个 $S_i$ i.n.s.，它通过 $S$ 作用 $n$ 次后的结果仍然是狭义可实现的，因为我们可以不断地添加间接神经元。如果 $S_1$ 与 $S_2$ 均狭义可实现，那么 $S^m S_1$ 与 $S^n S_2$ 均为i.n.s。现在考虑神经网络的四个基本构成组件 $S(p_1(z_1))$, $S(p_1(z_1)\lor p_2(z_1))$, $S(p_1(z_1)\land p_2(z_1))$ 与 $S(p_1(z_1)\land \lnot p_2(z_1))$（图七），它们都是狭义可实现的。

![[Pasted image 20240717180425.png]]
<center>图七：四个神经网络的基本构成组件</center>

根据已经得到的结论，$S^{m+n+1}(p_1(z_1))$, $S^{m+n+1}(p_1(z_1)\lor p_2(z_1))$, $S^{m+n+1}(p_1(z_1)\land p_2(z_1))$ 与 $S^{m+n+1}(p_1(z_1)\land \lnot p_2(z_1))$ 同样是狭义可实现的。根据广义可实现的定义，这些原命题都是广义可实现的。经过逻辑运算与合并，这些基本结构的组合可以在广义上实现更复杂的神经网络，也因此可以实现任何的TPE。

Theorem 3. *Let there be a complex sentence $S_1$ built up in any manner out of elementary sentences of the form $p(z_1-zz)$ where $zz$ is any numeral, by any of the propositional connections: negation, disjunction, conjunction, implication, and equivalence. Then $S_1$ is a* TPE *and only if it is false when its constituent $p(z_1-zz)$ are all assumed false-i.e. replaced by false sentences-or that the last line in its truth-table contains an 'F' -or there is no term in its Hilbert disjunctive normal form composed exclusively of negated terms.

定理三的内容看起来很复杂，但实际上讲的就是一个表达式成为TPE的条件；换言之，即是判定一个表达式是否为TPE的方法。定理三的证明过程在这里过多赘述。这样的方法有三个（它们实际上是等价的）：
1. 基本句子为假时复合句为假
2. 真值表的最后一行包含“假”
3. 希尔伯特析取标准型（HDNF）中没有全否定项

我们举几个例子。考虑以下复合句的判定：
1. 对于复合句 $S_1 = p(z_1 - 1) \land q(z_1 - 2)$，其中 $p(z_1 - 1)$ 和 $q(z_1 - 2)$ 是基本命题。当 $p(z_1 - 1)$ 和 $q(z_1 - 2)$ 都为假时，$S_1$ 也为假，即 $S_1 = \text{假} \land \text{假} = \text{假}$。因此，$S_1$ 满足第一个条件，$S_1$ 是 TPE。
2. 对于复合句 $S_2 = p(z_1 - 1) \lor q(z_1 - 2)$，构建其真值表如下：
![[Pasted image 20240717191842.png]]
可以看到，最后一行 $p(z_1 - 1) = \text{假}$ 和 $q(z_1 - 2) = \text{假}$ 时，$S_2$ 为假。因此，$S_2$ 满足第二个条件。
3. 考虑复合句 $S_3 = \neg p(z_1 - 1) \land \neg q(z_1 - 2)$，将其转换为 HDNF形式$S_3 = \neg p(z_1 - 1) \land \neg q(z_1 - 2)$。在 HDNF 中，唯一的项 $\neg p(z_1 - 1) \land \neg q(z_1 - 2)$ 是全由否定项组成的。因此，$S_3$ 不满足第三个条件，$S_3$ 不是 TPE。

我们现在开始通过应用这三个定理来实现一个神经网络。设想一个生活场景：当一个冰块在片刻内接触我们的皮肤再离开，我们会先感受到烫而不是凉；但如果这个冰块滞留更久的时间，就只会感受到一阵冷。现在，为了对这个情形建模，让$N_1$与$N_2$分别代表“热”与“冷”的接收器，再让$N_3$与$N_4$代表感知热与冷的神经元。我们假设，要是$N_4$要感受到冷，必须存在两个单位的突触延迟（冰凉的触感持续两个单位时间），于是可以得到下述句子：
$$\begin{align*}&N_3(t)\equiv N_1(t-1)\lor N_2(t-3)\land \lnot N_2(t-2) \\ 
&N_4(t)\equiv N_2(t-2)\land N_2(t-1)\end{align*}$$
由于这两个句子已经是希尔伯特析取标准形的形式，根据定理三，我们可以判定$N_3(t)$与$N_4(t)$ 均为TPE。基于这个前提，我们可以通过定理二来实现一个神经网络；首先利用函子 $S$ 将谓词所描述的状态均用图七展示的神经网络的四个基本组成部件写出：
$$\begin{align*}
&N_3(t)\equiv S\{N_1(t) \lor S[SN_2(t)\land \lnot N_2(t)]\} \\ &N_2(t)\equiv S[SN_2(t)\land N_2(t)] \end{align*}$$
为了简化复杂网络的构建，先从包含最多括号的项开始（神经元$c_2$的活动），然后向外拓展，递归构建完整的表达式。引入间接神经元 $c_a$，$c_2$ 到 $c_a$ 的网络形式是图七中的 a：
$$N_a(t)\equiv SN_2(t)$$
再处理包含第二多括号的项 $SN_2(t)\land \lnot N_2(t)$（它的形式为图七中的 d）。引入间接神经元 $c_b$: $$N_b(t)\equiv S[N_a(t)\land \lnot N_2(t)]$$最后，基于间接神经元的表述，可以写出以下两个句子：$$\begin{align*} N_3(t)\equiv S[N_1(t)\lor N_b(t)] \\ N_4(t)\equiv S[N_a(t)\land N_2(t)]\end{align*}$$这个形式无疑就是下图所示的神经网络。

![[Pasted image 20240717201211.png]]
在这个解题过程中，定理2提供了基础构建块和递归组合的方法，定理3的判定方法则保障了复合命题是TPE的前提。同时，间接神经元的引入使得这些组合更加灵活与可操作，确保每个步骤都能实现所需的逻辑功能，从而完成复杂的时间逻辑命题的实现。我们的结果清楚地表明了感知与“外部世界”之间的对应关系依赖于中间神经网络的特定结构特性。当然，这种错觉也可以在关于皮肤感受器行为的各种其他假设下产生，并且相应的神经网络会有所不同。

在无循环结构神经网络的理论中，还有一部分：（1）不同抑制现象的解释在广义上的等价的（2）Extinction与Learning与绝对的抑制现象等价。若能够证明这两个结论，就说明：当前提出的结构是对实际的神经网络一种合理的模拟；基于这些前提，我们的确能够计算任何网络的行为，并且确定呈现为某种状态的网络。这一部分的论证在原论文中占了很小的篇幅，因此在这里不过多赘述。

**小结**

MP神经元这一模型将神经元的行为简化为逻辑运算，通过“全或无”的响应方式执行基本的逻辑操作。它的重要意义在于将神经系统的行为形式化，证明了神经网络具有图灵完备性，可以模拟任何状态。这一突破性理论为后续的Perceptron（感知器）打下了坚实的基础，并且推动了人工智能神经网络（ANN）的发展。（画饼又不是饼的环节：不出意外的话，下一期的主题是感知器或者一些涌现相关的理论，在三天内就能出x（已经写完在编辑了所以不是饼啦！））

最后，啃古早论文不容易，所以痛并快乐着、水平不高的作者想求一个点赞(^-^)

**完**

**References**

[1] McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. _the Bulletin of Mathematical Biophysics_, _5_(4), 115–133. https://doi.org/10.1007/bf02478259
[2] Marshall, M. (2021, June 7). _Google has mapped a piece of human brain in the most detail ever_. New Scientist; New Scientist. https://www.newscientist.com/article/2279937-google-has-mapped-a-piece-of-human-brain-in-the-most-detail-ever/
[3] _A Friendly Introduction to [Deep] Neural Networks | KNIME_. (2021). KNIME. https://www.knime.com/blog/a-friendly-introduction-to-deep-neural-networks

‌