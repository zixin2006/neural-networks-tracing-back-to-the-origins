目的是学习单词序列的最佳的联合概率函数，同时学的不同的词是如何映射到对应的分布式特征向量上的。无监督的神经网络学习。

统计语言建模的一个目标是学习语言中单词序列的联合概率函数。作者通过学得语言的分布式表达来解决维度爆炸的问题（单词数目过多，如果要对所有单词建立一个很大的表来储存是不现实的）。语言的统计模型可以用给定前面所有单词的条件概率来表示下一个单词，也就是如下表达：
$$\hat{P}(w_1^T)=\prod_{t=1}^T \hat{P}(w_t|w_1^{t-1})$$
- $w_{t}$ 表示第 tt 个单词
- $w_j^i= = (x_{i},x_{i+1},...,x_{j})$

事实上，在单词序列中给定位置的单词对较近的单词更加依赖。考虑 n-gram 模型（$n = 3$ 的时候达到最好效果），构建下一个单词的条件概率可以只用前 $n-1$ 个单词来表达：
$$\hat{P}(w_t|w_1^{t-1})\approx \hat{P}(w_t|w_{t-n+1}^{t-1})$$
但是这种方法是有局限性的。一方面显然在最临近两个词之前的更多词也都隐含这语义信息，另一方面是这种方法没有考虑到词语间的相似性。（具体来说，在训练语料库中看到了the cat is walking in the bedroom这句话，应该可以帮助我们归纳出A dog was running in A room这句话的可能性几乎是一样的，因为狗和猫，The 和 a, room 和 bedroom 等有相似的语义和语法作用）

后续推导表达将都使用矩阵运算来进行，因此先说明一些本文中的标记：

- $v$ 为列向量，转置为 $v'$
- $A_{j}$ 表示矩阵的第 $j$ 行
- $x.y= x'y$

**Fighting the Curse of Dimensionality with Distributed Representations**

总的来看，提出的方法可以总结如下：

1. 对每个词表中的单词都找到其对应的分布式特征向量。（维度为 mm 的实值向量）
2. 将词序列中的词的特征向量表示为词序列的联合概率函数。
3. 同时学习单词特征向量和概率函数的参数。

**The Neural Model**

![](https://pic1.zhimg.com/v2-889375e875236b1177ff916ba891337a_1440w.jpg)

**变量标记以及总的过程**

- 词汇表 $V$
- 训练集是词的序列 $w_{1},w_{2},...,w_{T}$，每个单词都来自词汇表（此时是原始单词形式）
- 我们的目的是找到一个模型函数 $f(w_{t},w_{t-1},...,w_{t-n+1}) = P(w_{t}|w_{1}^{t-1}).$

可以把模型函数 $f(w_{t},w_{t-1},...,w_{t-n+1}) = P(w_{t}|w_{1}^{t-1})$ 分解成两个部分：

1. 找到一个映射集合 $C$ ，对于任意词汇表 $V$ 中的词汇 $i$ 都能通过 $C(i)$ 找到该词对应的特征向量。 CC 的大小为 $|V|\times m$，全部都是要训练的参数。
2. 基于单词、用 $C$ 表达的概率函数$$f(i,w_{t-1},...,w_{t-n+1})=g(i,C(w_{t-1},...,C(w_{t-n+1}))$$
总的来说 $f$ 函数是两个映射，即 $C$ 和 $g$ 的组合。$C$ 是由文本中所有单词共享的参数。故待确定的参数为$\theta = (C,w)$，后者为概率函数的模型参数。训练是通过最大化训练集中的对数损失函数来找到最优参数的，表达式如下：$$L=\frac{1}{T}\sum_t \log f(w_t,w_{t-1},...,w_{t-n+1};\theta)+R(\theta)$$$R(\theta)$ 为正则项。

**Description of Computation Process**

**前馈过程**

(a) Perform forward computation for the word features layer:
$$\begin{align*}&x(k)\leftarrow C(w_{t-k})\\ &x=(x(1),x(2),...x(n-1))\end{align*}$$
$x(k)$ 表示距离 $k-$th 单词的距离，是 $m$ 维向量。
(b) Perform forward computation for the hidden layer:
$$\begin{align*}&0\leftarrow d+Hx\\ &a\leftarrow \tanh(o)\end{align*}$$
隐含层的运算，注意

- 隐含层神经元的个数为 $h$
- $x$ 的大小为 $(n−1)m(n-1) m 的向量。_**（参数）**_
- HH 的大小为 h×(n−1)mh\times (n-1)m ，输出为大小为 hh 的向量（隐含层总输出，每个结点输出一个数）_**（参数）**_
- dd 的大小为 hh 的向量_**（参数）**_
- aa 为大小为 hh 的向量

![](https://pic4.zhimg.com/v2-9069a4c4eed60615f3d1f8c43cb72e01_1440w.jpg)

**输出层的运算**

ii 索引的的是词表 VV 中第几个单词，每个 yiy_{i} 是输出的前向计算的概率值。由于最后要经过 softmaxsoftmax 标准化，故计算了指数pip_{i}。

- UjU_{j} 的大小为 hh 的向量，（一共有 |V||V| 个）**_（参数）_**
- WjW_{j} 的大小为 mm 的向量（关于是否直接连接是可以通过训练来确定的，可以默认全都有直接连接，如果训练出来全都是0则为无连接情况）**_（参数）_**

![](https://pic2.zhimg.com/v2-a5fbfc0a93209c61c211ddcf4e4cb593_1440w.jpg)

总之就是得到每个词的概率值，然后通过这些概率值去计算更新对数似然函数（在上面有记录L的表达形式）（利用训练集的句子，每个句子是一个单词序列，可以计算对数似然函数）。

其实总的来说可以写成如下：

![](https://pic2.zhimg.com/v2-47968ebac246e7eaa80d0df1c56d5b35_1440w.png)

**利用深度学习工具：**

利用现在发达的深度学习工具可以非常快的解决上面的问题，本质上就是一个带有一层隐含层（激活函数为 tanhtanh ）和输出层的前向传播过程，同时加上一个直接从原始输入 xx 连接到输出端的输出层。但是注意这里的输入层也是要训练的参数。

- xx 的大小为 (n−1)m(n-1) m 的向量。**_（参数待训练）_**
- 中间层也就是隐含层的神经元个数为 h，激活函数为 tanhtanh
- 输出层有 |V||V| 个神经元

**反向传播**

![](https://pic4.zhimg.com/v2-732438dd73f62594d0e343cc68863c6f_1440w.png)

![](https://pic2.zhimg.com/v2-49d1d746c197866e3210da2c2c81d1d9_1440w.jpg)

链式法则更新待训练参数 、、b、W、Ub、W、U ，同时计算 ∂L∂a\frac{\partial L}{\partial a}

![](https://pic3.zhimg.com/v2-78af72de4b196f502ee1d8d28f14b7c4_1440w.jpg)

计算更新 、d、Hd、H

![](https://pic1.zhimg.com/v2-e2655a5e5d96f317e3b012dbf0ba57da_1440w.jpg)

计算更新最关键的映射 CC